# -*- coding: utf-8 -*-
"""rf_classification

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UlVQ54E0qf_Uv3T8U_fG7FS2mrnCs3Kd
"""

import pandas as pd
import numpy as np
import os
import joblib
import gc
import json
import time
from datetime import datetime

from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OrdinalEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score, f1_score, precision_score, recall_score,
    confusion_matrix, roc_auc_score, log_loss, average_precision_score
)

# --- Configuration ---
CLASS_TARGET = 'DEP_DEL15'
LEAKAGE_COLS = ['CARRIER_DELAY', 'WEATHER_DELAY', 'NAS_DELAY',
                'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY']

# Define Categoricals (Same as before)
CATEGORICAL_COLS = [
    # Core IDs
    'CARRIER_NAME',
    'DEPARTING_AIRPORT',
    'PREVIOUS_AIRPORT',
    'DESTINATION_AIRPORT',
    # Engineered Routes
    'ROUTE_NAME',      # Origin -> Dest
    'INCOMING_ROUTE',  # Prev -> Origin
    'CARRIER_AIRPORT', # Hub Effect
    # Time
    'DEP_TIME_BLK',
    'MONTH',
    'DAY_OF_WEEK',
    'SEASON',
    # Groups
    'DISTANCE_GROUP',
    'SEGMENT_NUMBER',
    # Weather Flags / binary flags
    'IS_HEAVY_RAIN', 'IS_SNOWY',
    'IS_FREEZING', 'IS_EXTREME_HEAT',
    'AWND_missing', 'TMIN_missing', 'TMAX_missing',
]
# Add WT flags
CATEGORICAL_COLS.extend([f'WT{str(i).zfill(2)}' for i in range(1, 12)])


# --- Transformers ---
class ColumnDropper(BaseEstimator, TransformerMixin):
    """Drop known leakage columns."""
    def __init__(self, columns_to_drop):
        self.columns_to_drop = columns_to_drop

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        return X.drop(
            columns=[c for c in self.columns_to_drop if c in X.columns],
            errors="ignore"
        )


def get_classifier_pipeline():
    """
    Returns a sklearn Pipeline that:
      1. Drops leakage columns
      2. Ordinal-encodes categorical columns
      3. Passes all remaining columns through as numeric features
    """
    preprocessor = ColumnTransformer(
        transformers=[
            (
                "cat",
                OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=-1),
                CATEGORICAL_COLS
            )
        ],
        remainder="passthrough"  # keep non-categorical columns as-is
    )

    pipeline = Pipeline(steps=[
        ("dropper", ColumnDropper(LEAKAGE_COLS)),
        ("preprocessor", preprocessor),
    ])

    return pipeline

class InferencePipeline:
    """
    Simple wrapper to keep preprocessor + model together
    and expose predict / predict_proba.
    """
    def __init__(self, preprocessor, model):
        self.preprocessor = preprocessor
        self.model = model

    def predict(self, X):
        X_trans = self.preprocessor.transform(X)
        return self.model.predict(X_trans)

    def predict_proba(self, X):
        X_trans = self.preprocessor.transform(X)
        return self.model.predict_proba(X_trans)

# best_model_params = {'n_estimators': 200, 'n_bins': 128, 'min_samples_split': 5, 'max_features': 'log2', 'max_depth': 15}


# --- Training Function ---
def train_classifier(
    X_train_path,
    y_train_path,
    X_test_path,
    y_test_path,
    description="RF_Classifier",
    checkpoint_dir=None,
    **model_params
):
    """
    Train a RandomForestClassifier with the same structure as your old LGBM pipeline.
    - Loads CSVs
    - Builds preprocessing pipeline
    - Trains RF with stratified validation split
    - Evaluates on test set
    - Saves InferencePipeline + metrics JSON (if checkpoint_dir is provided)
    """

    print(f"\n--- Starting Classifier Training: {description} ---")

    # 1. Load Data
    print("  Loading Training Data...")
    X_train = pd.read_csv(X_train_path)
    y_train = pd.read_csv(y_train_path)[CLASS_TARGET].squeeze()

    # 2. Stratified Split for Train and Validation Sets
    print("  Creating Stratified Validation Split (15%)...")
    X_tr, X_val, y_tr, y_val = train_test_split(
        X_train,
        y_train,
        test_size=0.15,
        random_state=42,
        stratify=y_train,
    )
    del X_train, y_train
    gc.collect()

    # 3. Build Pipeline (drop leakage + ordinal encoding)
    pipeline = get_classifier_pipeline()

    # 4. Build Random Forest model
    rf_default_params = {
        "n_estimators": 200,
        "max_depth": 15,
        "max_features": "log2",
        "min_samples_split": 5,
        "min_samples_leaf": 5,
        "n_jobs": -1,
        "class_weight": "balanced",
        "random_state": 42,
    }
    rf_default_params.update(model_params)
    model = RandomForestClassifier(**rf_default_params)

    # 5. Preprocess & Fit
    print("  Preprocessing & Fitting RandomForest...")
    start = time.time()
    X_tr_trans = pipeline.fit_transform(X_tr, y_tr)
    X_val_trans = pipeline.transform(X_val)

    model.fit(X_tr_trans, y_tr)
    train_time = time.time() - start
    print(f"  Training finished in {train_time:.1f}s")

    del X_tr, X_val, X_tr_trans, X_val_trans, y_tr, y_val
    gc.collect()

    # 6. Evaluation on Test Set
    print("  Evaluating on Test Set...")
    X_test = pd.read_csv(X_test_path)
    y_test = pd.read_csv(y_test_path)[CLASS_TARGET].squeeze()

    X_test_trans = pipeline.transform(X_test)
    y_pred = model.predict(X_test_trans)
    y_prob = model.predict_proba(X_test_trans)[:, 1]

    # 7. Report Metrics
    results = {
        "description": description,
        "accuracy": accuracy_score(y_test, y_pred),
        "auc": roc_auc_score(y_test, y_prob),
        "average_precision/auprc": average_precision_score(y_test, y_prob),
        "log_loss": log_loss(y_test, y_prob),
        "f1": f1_score(y_test, y_pred),
        "precision": precision_score(y_test, y_pred),
        "recall": recall_score(y_test, y_pred),
        "confusion_matrix": confusion_matrix(y_test, y_pred).tolist(),
        "training_time": train_time,
        "parameters": model.get_params(),
    }

    print(f"  AUC: {results['auc']:.4f}, LogLoss: {results['log_loss']:.4f}")
    print(f"  Precision: {results['precision']:.4f}, Recall: {results['recall']:.4f}")

    # 8. Save Metrics and Pipeline
    if checkpoint_dir:
        os.makedirs(checkpoint_dir, exist_ok=True)
        safe_desc = "".join([c if c.isalnum() else "_" for c in description])
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")

        # Save full inference pipeline (preprocessing + RF model)
        path = os.path.join(checkpoint_dir, f"RF_CLS_{safe_desc}_{ts}.joblib")
        joblib.dump(InferencePipeline(pipeline, model), path)
        results["checkpoint_path"] = path

        # Save metrics
        metrics_path = path.replace(".joblib", "_metrics.json")
        with open(metrics_path, "w") as f:
            json.dump(results, f, indent=4)

    return results