# -*- coding: utf-8 -*-
"""rf_regressor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VOVsnv4KfwgcOx5AxpvppLFqf2ghCRU9
"""

import pandas as pd
import numpy as np
import os
import joblib
import gc
import json
import time
from datetime import datetime

from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OrdinalEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import (
    root_mean_squared_error, mean_absolute_error, r2_score,
    mean_tweedie_deviance, mean_gamma_deviance, mean_absolute_percentage_error
)

# --- Configuration ---
REGR_TARGET = 'DEP_ADDED_DELAY'
LEAKAGE_COLS = ['CARRIER_DELAY', 'WEATHER_DELAY',
                'NAS_DELAY', 'SECURITY_DELAY',
                'LATE_AIRCRAFT_DELAY']

CATEGORICAL_COLS = [
    'CARRIER_NAME', 'DEPARTING_AIRPORT', 'PREVIOUS_AIRPORT', 'DESTINATION_AIRPORT',
    'ROUTE_NAME', 'INCOMING_ROUTE', 'CARRIER_AIRPORT',
    'DEP_TIME_BLK', 'MONTH', 'DAY_OF_WEEK', 'SEASON',
    'DISTANCE_GROUP', 'SEGMENT_NUMBER',
    'IS_HEAVY_RAIN', 'IS_SNOWY', 'IS_FREEZING', 'IS_EXTREME_HEAT',
    'AWND_missing', 'TMIN_missing', 'TMAX_missing',
]
CATEGORICAL_COLS.extend([f'WT{str(i).zfill(2)}' for i in range(1, 12)])


# --- Transformers ---
class ColumnDropper(BaseEstimator, TransformerMixin):
    def __init__(self, columns_to_drop):
        self.columns_to_drop = columns_to_drop

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        return X.drop(columns=[c for c in self.columns_to_drop if c in X.columns])


def get_regressor_pipeline():
    """
    Drop leakage columns, then ordinal-encode categorical features and
    passthrough the rest.
    """
    preprocessor = ColumnTransformer(
        transformers=[
            (
                "cat",
                OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=-1),
                CATEGORICAL_COLS,
            ),
        ],
        remainder="passthrough",  # keep non-categorical columns as-is
    )

    pipe = Pipeline(steps=[
        ('dropper', ColumnDropper(LEAKAGE_COLS)),
        ('preprocessor', preprocessor),
    ])
    return pipe


class InferencePipeline:
    def __init__(self, preprocessor, model):
        self.preprocessor = preprocessor
        self.model = model
    def predict(self, X):
        X_trans = self.preprocessor.transform(X)
        return self.model.predict(X_trans)

    def predict_proba(self, X):
        X_trans = self.preprocessor.transform(X)
        return self.model.predict_proba(X_trans)

# --- Training Function ---
def train_regressor(
    X_train_path,
    y_train_path,
    X_test_path,
    y_test_path,
    stopping_rounds=100,  # kept for API compatibility, not used by RF
    description="RF_Regressor",
    pos_only=False,
    extra_metrics=None,
    checkpoint_dir=None,
    **model_params,
):
    """
    Train a RandomForestRegressor with your flight-delay features.

    - Optionally trains only on rows with DEP_ADDED_DELAY > 0 (pos_only=True)
    - Uses a 15% validation split (for monitoring only)
    - Saves an InferencePipeline (preprocessor + RF model) plus metrics JSON
    """

    print(f"\n--- Starting Regressor Training: {description} ---")

    # 1. Load Data
    print("  Loading Training Data...")
    X_train = pd.read_csv(X_train_path)
    y_train = pd.read_csv(y_train_path)[REGR_TARGET].squeeze()

    if pos_only:
        print(f"  Filtering for positive delays... (Original: {len(X_train)})")
        mask_pos = (y_train > 0)
        X_train = X_train[mask_pos]
        y_train = y_train[mask_pos]
        print(f"  Training samples remaining: {len(X_train)}")
    else:
        print("  pos_only is False, training on WHOLE training set")

    # 2. Split
    print("  Creating Validation Split (15%)...")
    X_tr, X_val, y_tr, y_val = train_test_split(
        X_train,
        y_train,
        test_size=0.15,
        random_state=42,
    )
    del X_train, y_train
    gc.collect()

    # 3. Pipeline Setup
    pipeline = get_regressor_pipeline()

    # 4. Build RandomForestRegressor
    # rf_params = {
    #     "n_estimators": 120,
    #     "max_depth": 10,
    #     "max_features": "sqrt",
    #     "min_samples_split": 5,
    #     "min_samples_leaf": 2,
    #     "n_jobs": -1,
    #     "random_state": 42,
    #     "criterion":"poisson"
    #     # "bootstrap": True
    # }
    
    rf_params = {
        "n_estimators": 300,
        "max_depth": None,
        "max_features": "sqrt",
        "min_samples_split": 2,
        "min_samples_leaf": 1,
        "n_jobs": -1,
        "random_state": 42,
        "criterion":"poisson"
    }
    rf_params.update(model_params)
    model = RandomForestRegressor(**rf_params)

    print("  Preprocessing & Fitting RandomForestRegressor...")
    X_tr_trans = pipeline.fit_transform(X_tr, y_tr)
    X_val_trans = pipeline.transform(X_val)

    start = time.time()
    model.fit(X_tr_trans, y_tr)
    train_time = time.time() - start
    print(f"  Training finished in {train_time:.1f}s")

    del X_tr, X_val, X_tr_trans, X_val_trans, y_tr, y_val
    gc.collect()

    # --- Save model first ---
    results = {
        "description": description,
        "training_time": train_time,
        "parameters": model.get_params(),
    }

    if checkpoint_dir:
        os.makedirs(checkpoint_dir, exist_ok=True)
        safe_desc = "".join([c if c.isalnum() else "_" for c in description])
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")

        path = os.path.join(checkpoint_dir, f"{safe_desc}_{ts}.joblib")
        print(f"  Saving model to: {path} ...")
        joblib.dump(InferencePipeline(pipeline, model), path)
        results["checkpoint_path"] = path

    # 5. Evaluate
    print("  Evaluating on Test set examples...")
    try:
        X_test = pd.read_csv(X_test_path)
        y_test = pd.read_csv(y_test_path)[REGR_TARGET].squeeze()

        if pos_only:
            print("  pos_only is True, testing on only positive examples")
            mask_test_pos = (y_test > 0)
            X_test = X_test[mask_test_pos]
            y_test = y_test[mask_test_pos]
        else:
            print("  pos_only is False, testing on WHOLE test set")

        X_test_trans = pipeline.transform(X_test)
        y_pred = model.predict(X_test_trans)

        y_pred_safe = np.maximum(y_pred, 1e-9)

        results.update({
            "rmse": root_mean_squared_error(y_test, y_pred),
            "mae": mean_absolute_error(y_test, y_pred),
            "mape": mean_absolute_percentage_error(y_test, y_pred),
            "r2": r2_score(y_test, y_pred),
        })

        # Advanced metrics
        try:
            results["gamma_deviance"] = mean_gamma_deviance(y_test, y_pred_safe)
        except Exception as e:
            print(f"  Warning: Gamma deviance calc failed: {e}")

        try:
            results["tweedie_deviance"] = mean_tweedie_deviance(
                y_test, y_pred_safe, power=1.5
            )
        except Exception as e:
            print(f"  Warning: Tweedie deviance calc failed: {e}")

        # Extra custom metrics
        if extra_metrics:
            for name, func in extra_metrics.items():
                try:
                    results[name] = func(y_test, y_pred)
                except Exception as e:
                    print(f"  Warning: Custom metric '{name}' failed: {e}")

        print(f"  RMSE:  {results.get('rmse', 'N/A'):.4f}")
        print(f"  MAE:   {results.get('mae', 'N/A'):.4f}")
        print(f"  MAPE:  {results.get('mape', 'N/A'):.4%}")

        # Save Metrics JSON
        if checkpoint_dir and "checkpoint_path" in results:
            json_path = results["checkpoint_path"].replace(".joblib", "_metrics.json")
            with open(json_path, "w") as f:
                json.dump(results, f, indent=4)
            print(f"  Metrics saved to: {json_path}")

    except Exception as e:
        print("\n!!! CRITICAL WARNING: Evaluation crashed, but Model IS SAVED !!!")
        print(f"Error details: {e}")

    return results